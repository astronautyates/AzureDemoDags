{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb5816e-442d-4ea4-9edb-f3c36b928270",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"containername\"\n",
    "storage_account_access_key = \"accesskey\"\n",
    "file_location = \"wasbs://mlstorage@\" + storage_account_name + \".blob.core.windows.net/PreppedMLData.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.\" + storage_account_name + \".blob.core.windows.net\",\n",
    "  storage_account_access_key)\n",
    "\n",
    "remote_table = spark.read.format(file_type).option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(file_location)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15116ee-5d3e-4cfc-b2cd-876ab0f79f55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d345ae3bb8048a195109be64ccc6224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c96275c1e646988e5d9e40818dd54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'remote_table' is a Spark DataFrame available in your Databricks environment\n",
    "\n",
    "# Drop the columns with all null values\n",
    "columns_to_drop = ['Satisfactionscore', 'CardType', 'PointEarned']\n",
    "remote_table = remote_table.drop(*columns_to_drop)\n",
    "\n",
    "# Handle remaining missing values\n",
    "remote_table = remote_table.na.drop()\n",
    "\n",
    "# Categorical columns\n",
    "categorical_cols = ['Geography', 'Gender']\n",
    "\n",
    "# String Indexing for categorical features\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_indexed\") for column in categorical_cols]\n",
    "\n",
    "# Numeric columns for scaling\n",
    "numeric_cols = [\"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"EstimatedSalary\"]\n",
    "\n",
    "# VectorAssembler for combining feature columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_cols,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# MinMaxScaler for feature scaling\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Pipeline for the tasks\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
    "\n",
    "# Fit and Transform the pipeline on the dataframe\n",
    "transformed_df = pipeline.fit(remote_table).transform(remote_table)\n",
    "\n",
    "# Selecting features and target variable\n",
    "final_df = transformed_df.select(\"scaledFeatures\", \"CreditScore\")\n",
    "\n",
    "# Convert to Pandas DataFrame for Scikit-learn compatibility\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "pandas_df = final_df.toPandas()\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X = pd.DataFrame(pandas_df[\"scaledFeatures\"].tolist())\n",
    "y = pandas_df[\"CreditScore\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you can use Scikit-learn for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132b453a-4a0a-432a-a3cf-d845acf2e7e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf33aaf145d4d55a105e1e873a9a4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 10391.90274\n",
      "Root Mean Squared Error (RMSE): 101.94068245798631\n",
      "R-squared (R2 ): -0.25139004881514504\n",
      "    Actual Credit Score  Predicted Credit Score\n",
      "0                   758                  700.92\n",
      "1                   493                  561.29\n",
      "2                   479                  655.16\n",
      "3                   813                  633.50\n",
      "4                   738                  555.15\n",
      "5                   603                  601.62\n",
      "6                   722                  655.30\n",
      "7                   625                  553.92\n",
      "8                   653                  595.69\n",
      "9                   619                  560.01\n",
      "10                  574                  609.93\n",
      "11                  637                  644.92\n",
      "12                  668                  669.00\n",
      "13                  601                  580.76\n",
      "14                  663                  773.44\n",
      "15                  822                  596.79\n",
      "16                  699                  703.36\n",
      "17                  656                  659.83\n",
      "18                  726                  584.71\n",
      "19                  776                  728.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'final_df' is your PySpark DataFrame that includes 'scaledFeatures' and 'creditscore'\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "pandas_df = final_df.toPandas()\n",
    "\n",
    "# Convert the DenseVector to lists\n",
    "pandas_df['scaledFeatures'] = pandas_df['scaledFeatures'].apply(lambda x: x.toArray().tolist())\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X = pd.DataFrame(pandas_df['scaledFeatures'].tolist())\n",
    "y = pandas_df['CreditScore']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R2 ): {r2}\")\n",
    "\n",
    "# Create a DataFrame with actual and predicted values\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual Credit Score': y_test,\n",
    "    'Predicted Credit Score': predictions\n",
    "})\n",
    "\n",
    "# Reset index to avoid index-related issues\n",
    "results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Convert the Pandas DataFrame to a PySpark DataFrame\n",
    "results_spark_df = spark.createDataFrame(results_df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLNotebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
